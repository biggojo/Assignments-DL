{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "64116a8d660041ad995eee80e9ccbd43",
        "deepnote_cell_height": 153.171875,
        "deepnote_cell_type": "markdown",
        "id": "WExusxG_2tgY",
        "tags": []
      },
      "source": [
        "#### This submission is for... (*put up to three people*)\n",
        "- Abraham Gassama (2285843)\n",
        "- Zhuo Le Lee (2317240)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b471288db3b44cfb992b6c07f7c3fd83",
        "deepnote_cell_type": "text-cell-h1",
        "formattedRanges": [],
        "id": "w6spdQYQ2tgd",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "# Exercise 1 - Simple Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7b2ab899b012471f80f00f1ff3b7992a",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "xWJ_eyKS2tge",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "In the first exercise, you will incrementally build a simple neural network from scratch with numpy. Our first challenge is solving the XOR task that you've seen in the lecture, before we move to a slightly more complex problem, namely the Iris dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "38a559d784cb43709780881a4c56aedd",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "fDTGjoEB2tge",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "You can receive up to three points for your implementation of Exercise 1. After that you can either choose Exercise 2A or Exercise 2B to receive another three points. In sum, you can get up to six bonus points for the exam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6984113451a342818dac8a8e0bd440ac",
        "deepnote_cell_height": 94.78125,
        "deepnote_cell_type": "markdown",
        "id": "VGOlYN9Z2tgf",
        "tags": []
      },
      "source": [
        "- **Exercise 2A**: Building a Transformer network with PyTorch applied in the NLP domain\n",
        "- **Exercise 2B**: Building a GAN network with PyTorch applied in the image domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dfb46b3d43444b14961ecd9967845ea1",
        "deepnote_cell_height": 141.953125,
        "deepnote_cell_type": "markdown",
        "id": "pOIoDJv_2tgf",
        "tags": []
      },
      "source": [
        "**Important Notice**: Throughout the notebook, basic structures are provided such as functions and classes without bodies or partial bodies, and variables that you need to assign to. **Don't change the names of functions, variables, and classes - and make sure that you are using them!** You're allowed to introduce helper variables and functions. Occasionally, we use **type annotations** that you should follow. They are not enforced by Python. Whenenver you see an ellipsis `...` or TODO comment, you're supposed to insert code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "10a3e57eb4844285b70b25bcd41c834e",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "M8QD3-q02tgg",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## XOR Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "84d575c151ba447192c5bc8d6bf696e1",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "B4XIvrKj2tgh",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "XOR (exclusive OR) is a logic function that gives 1 as an output when the number of true inputs is odd, otherwise it outputs a 0. Our goal is to model this function using neurons. We'll start with a single neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c9900820a8204251a55585f98126c98e",
        "deepnote_cell_height": 276.84375,
        "deepnote_cell_type": "markdown",
        "id": "ekh7hOIk2tgh",
        "tags": []
      },
      "source": [
        "<center><img src=\"https://community.anaplan.com/t5/image/serverpage/image-id/29631i3AA6C01377A8550F/image-size/large?v=v2&px=999\" width=\"250\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "64f8dfc7d36a4abfa267750813edfa2d",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "txWXYqry2tgi",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## A Single Neuron (Perceptron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "eeef0f122a2e45be9ab17634e036b17f",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "RS83rXR82tgi",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "Let's start with importing some necessary dependencies that we will need throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "cell_id": "95e9b1a1f9494ea89f65aee06be516dc",
        "deepnote_cell_height": 61,
        "deepnote_cell_type": "code",
        "id": "-nrjhMBO2tgj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "39505416089744699256a5844dd99d19",
        "deepnote_cell_height": 252.9375,
        "deepnote_cell_type": "markdown",
        "id": "dlGxHaM72tgk",
        "tags": []
      },
      "source": [
        "In the first part of this exercise you'll build a perceptron, a single neuron, that takes both binary input values and returns a binary output value.\n",
        "\n",
        "<center><img src=\"https://i.stack.imgur.com/eBSki.jpg\" width=\"280\" />\n",
        "\n",
        "<center><img src=\"\" width=\"280\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "816aceffd3b045029c7d82becc35489e",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "I7PmyXz92tgk",
        "tags": []
      },
      "source": [
        "Perceptron can be seen as a single neuron, mapping an input $\\textbf{x}$ to an output $o$ using weights $\\textbf{w}$ and a bias $b$. $\\cdot$ is the dot product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "11a6fcabddb143599d0b7bc55f5cc50c",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "7UjfCaeq2tgl",
        "tags": []
      },
      "source": [
        "$o = \\textbf{w}\\cdot \\textbf{x}+b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "59d21d8a14cf4ffab7daf30a94c91be8",
        "deepnote_cell_height": 110.78125,
        "deepnote_cell_type": "markdown",
        "id": "QW5c6tT52tgl",
        "tags": []
      },
      "source": [
        "#### Perceptron Update Rule\n",
        "\n",
        "In the lecture we learned about the **Perceptron algorithm** / **Perceptron update rule** which we can apply to binary classification problems. Let's use it here to have a first baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fe3ac47eb7134fbd9e92b9b887dcda12",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "5dNqpPdd2tgl",
        "tags": []
      },
      "source": [
        "For classification problems $0>o$ is interpreted as class 1, and $o<0$ is interpreted as class 0. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6454ee77e7e4411f8d341729e17e90a5",
        "deepnote_cell_height": 262.734375,
        "deepnote_cell_type": "markdown",
        "id": "CWqEU0Ds2tgl",
        "tags": []
      },
      "source": [
        "For updating the associated weights, we can use the following update rule:\n",
        "\n",
        "$w_i = w_i + \\nabla w_i$\n",
        "\n",
        "where\n",
        "\n",
        "$\\nabla w_i = \\eta(t-o)x_i$\n",
        "\n",
        "- $t$ is the target\n",
        "- $o$ is the output\n",
        "- $\\eta$ is the learning rate (a small constant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "89280b788ec34866ab8f8ead1cc686c3",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "oCy6z6Ig2tgm",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Implementation of a Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "cell_id": "82eb15fa556f4017824bfc25bdfa9b71",
        "deepnote_cell_height": 871,
        "deepnote_cell_type": "code",
        "id": "eNs7zHyT2tgm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class perceptron_implementation():\n",
        "    def __init__(self):\n",
        "        self.neuron_weights = None\n",
        "        self.bias = None\n",
        "        self.initialize_weights()\n",
        "        \n",
        "    def initialize_weights(self):\n",
        "        # TODO: \n",
        "        # Initialize weights \n",
        "        # For perceptrons, it's possible to initialize the weights with 0\n",
        "        self.neuron_weights = np.array([0.0, 0.0])\n",
        "        self.bias = 1.0\n",
        "        # END TODO\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        # TODO\n",
        "        # Implement forward propagation\n",
        "        # x = hilfsvariable für update rule\n",
        "        output = None\n",
        "        output = self.neuron_weights[0]*x[0] + self.neuron_weights[1]*x[1] + self.bias\n",
        "        \n",
        "        if output > 0.0 :\n",
        "          output = 1.0\n",
        "        elif output < 0.0 :\n",
        "          output = 0.0\n",
        "\n",
        "        \n",
        "        # END TODO\n",
        "        return output, x\n",
        "\n",
        "    def perceptron_update_rule(self, target, prediction, input_perceptron_ur, learning_rate = 1):\n",
        "        # TODO\n",
        "        # Perform perceptron update rule that is defined above\n",
        "        # use self.neuron_weights\n",
        "        new_weights = None      \n",
        "        update_delta = (learning_rate*(target - prediction))*input_perceptron_ur\n",
        "        print(update_delta)\n",
        "        new_weights = self.neuron_weights + update_delta\n",
        "        # END TODO\n",
        "        self.neuron_weights = new_weights\n",
        "\n",
        "    def train(self, input_data, targets):\n",
        "        \"\"\"\n",
        "        input_data: Multi-dimensional array that contains all inputs\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Call the necessary functions to train a single neuron for the given task\n",
        "        # Complete the rest of the code to correctly train the model\n",
        "\n",
        "        output_tr = None\n",
        "        input_tr = None\n",
        "\n",
        "        for i in range(len(input_data)) : \n",
        "          output_tr, input_tr  =  self.forward_pass(input_data[i])\n",
        "          self.perceptron_update_rule(targets[i], output_tr, input_tr, learning_rate = 1)\n",
        "        # END TODO\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        # TODO\n",
        "        # Test the trained neuron\n",
        "        # do to the return of the forward pass we store the second return in a variable which isnt returned \n",
        "        output = []\n",
        "        var1 = None\n",
        "        var2 = None\n",
        "\n",
        "        \n",
        "        for i in range(len(input_data)) : \n",
        "          var1, var2 = self.forward_pass(input_data[i])       \n",
        "          output.append(var1)\n",
        "\n",
        "        output = np.asarray(output)\n",
        "\n",
        "        # END TODO\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "75e4622dce1a4d39b1734879909d04d2",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "5kpPuTNY2tgn",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "cell_id": "fdac76e29da047579c02f7c61cec4ffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_height": 223,
        "deepnote_cell_type": "code",
        "id": "k7UXXkGL2tgn",
        "outputId": "8cb3ffcc-5a52-4fda-aba2-55fe8ebcc13d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0. -0.]\n",
            "[0. 0.]\n",
            "[0. 0.]\n",
            "[-1. -1.]\n"
          ]
        }
      ],
      "source": [
        "perceptron = perceptron_implementation()\n",
        "\n",
        "# TODO\n",
        "\n",
        "input_data = np.array([(0,0) , (1,0) , (0,1) , (1,1)], dtype=float)\n",
        "targets = np.array([(0), (1), (1), (0)], dtype = float)\n",
        "\n",
        "\n",
        "# train the corresponding single neuron \n",
        "perceptron.train(input_data, targets)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c22027433597480481df2737709639dd",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "TD9gtzyL2tgn",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "cell_id": "c454d52152ce428c81d51ad9f34fd970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_height": 151,
        "deepnote_cell_type": "code",
        "id": "agv_oJ382tgn",
        "outputId": "b1530547-75bd-4d40-e90a-01a80269f4f5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# Test the trained model\n",
        "\n",
        "predictions = perceptron.inference(input_data)\n",
        "print(predictions)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "85b73e5dbdd14488b836e0dad804b0a3",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "DcMjb5zH2tgo",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e76ebc19dbad44988015c11187bd7969",
        "deepnote_cell_height": 243.34375,
        "deepnote_cell_type": "markdown",
        "id": "ETEW4Rhp2tgo",
        "tags": []
      },
      "source": [
        "For evaluation, we will need to consider appropriate metrics. For classification tasks, **accuracy** is one of the most common metrics.\n",
        "\n",
        "It is defined as:\n",
        "\n",
        "$\\textrm{Accuracy}=\\frac{1}{N}\\sum_i^N1(y_i=\\hat{y}_i)$\n",
        "\n",
        "where $y$ is an array of our target values, and $\\hat{y}$ is an array of our predictions.\n",
        "\n",
        "For accuracy, if outputs are probabilities, there needs to be a threshold for transforming logit predictions to binary `(0,1)` predictions. We will set this threshold to `0.5`. For our perceptron this is not needed, since we already output binary values, however, we will use the `accuracy` function later on, so the predictions should be considered to be probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "cell_id": "65abd9e35d404aa5af91b1ab7d74147b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_height": 259,
        "deepnote_cell_type": "code",
        "id": "xJCPWPYj2tgo",
        "outputId": "e5179182-2415-4213-d956-2121ff8239f5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.25\n"
          ]
        }
      ],
      "source": [
        "def accuracy(predictions: np.ndarray, targets: np.ndarray, threshold=0.5) -> float:\n",
        "    # TODO\n",
        "    # Implement the accuracy metric\n",
        "    # As we have different dimensions in the iris task then the rest implemented first a funcion which converts the targets into one hot encodings first and then performed\n",
        "    # the first case is the iris case, the second is the rest\n",
        "    # i could have converted the the one hot encodings back to the ints for each classes but i already did this \n",
        "    if len(targets)>5 :\n",
        "\n",
        "      prediction_copy = predictions\n",
        "      for i in range(len(predictions)):\n",
        "        copy = predictions[i]\n",
        "        max_value = np.max(copy)\n",
        "        for j in range (len(predictions[0])):\n",
        "\n",
        "            if copy[j] == max_value:\n",
        "                copy[j] = 1.\n",
        "            else: \n",
        "                copy[j] = 0.\n",
        "        prediction_copy[i] = copy\n",
        "\n",
        "      help_value = 0.0\n",
        "\n",
        "      for i in range(len(predictions)):\n",
        "        if (prediction_copy[i] == targets[i]).all() == True:\n",
        "            help_value = help_value + 1\n",
        "\n",
        "      output = help_value/len(predictions)\n",
        "      return output, prediction_copy\n",
        "\n",
        "    else: \n",
        "      for i in range(len(targets)) : \n",
        "        if targets[i] > 0.5 : \n",
        "          targets[i] = 1.0\n",
        "        else: \n",
        "          targets[i] = 0.0\n",
        "\n",
        "        if predictions[i] > 0.5 : \n",
        "          predictions[i] = 1.0\n",
        "        else: \n",
        "          predictions[i] = 0.0\n",
        "      help_value = 0.0\n",
        "      for i in range(len(targets)):\n",
        "        if targets[i] == predictions[i] : \n",
        "          help_value += 1\n",
        "\n",
        "      accuracy_value = help_value/len(targets)\n",
        "             \n",
        "    \n",
        "      return accuracy_value\n",
        "\n",
        "# END TODO\n",
        "\n",
        "# TODO\n",
        "# Call accuracy function and provide necessary inputs to calculate accuracy\n",
        "accuracy_value = accuracy(predictions, targets)\n",
        "print(accuracy_value)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cbdff3e9b9014471a55b2ea4d1221784",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "YZJLPvp22tgo",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Multiple Neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd42101ad8624579bd3ba87011aace0d",
        "deepnote_cell_height": 74.78125,
        "deepnote_cell_type": "markdown",
        "id": "d-OvFWl22tgo",
        "tags": []
      },
      "source": [
        "The perceptron algorithm can't be generalized to multiple neurons or even layers of neurons, that's why we will now use **backpropagation**. This requires us to have a **loss function**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a5b08e555e0c4ff6b726f6e609bb8dc9",
        "deepnote_cell_height": 211.0625,
        "deepnote_cell_type": "markdown",
        "id": "I6J7e8Fh2tgo",
        "tags": []
      },
      "source": [
        "For our XOR task, it is now our goal is now to build a network akin to this, i.e., a network with three single-neuron hidden layers:\n",
        "\n",
        "<img src=\"https://i.imgur.com/oErVmm2.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aaab44ed0b0948398d7bc4258dee27ab",
        "deepnote_cell_height": 267.75,
        "deepnote_cell_type": "markdown",
        "id": "jc6g4n0-2tgp",
        "tags": []
      },
      "source": [
        "#### Backpropagation\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/LgBzpYD.png\" width=\"400\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "207132d07fec4fe887a8ce578307e95a",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "NHAGXyrR2tgp",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Sigmoid Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b75c8af994a84628af79cb6906e06842",
        "deepnote_cell_height": 97.171875,
        "deepnote_cell_type": "markdown",
        "id": "y-LgMkeC2tgp",
        "tags": []
      },
      "source": [
        "For a binary classification problem, we can use the sigmoid activation function in the output layer which outputs values in the range of 0 and 1. So, for a positive case (class 1), we can interpret $p_1 = \\sigma(o)$ as the probability of that class, while $p_0 = 1 - p_1$ can be seen the probability of the negative case (class 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "cell_id": "f3ee1efc24284dfc836bb18198f6a1e8",
        "deepnote_cell_height": 295,
        "deepnote_cell_type": "code",
        "id": "p9nIIYj02tgp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class sigmoid_activation_function():\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement Sigmoid function for the input_data\n",
        "        output = 1.0/(1.0 + math.exp(-input_data))\n",
        "        # END TODO\n",
        "        return output\n",
        "    \n",
        "    def backward(self, gradients):\n",
        "        # calculate the gradients with help of the derivative\n",
        "        derivate_sigmoid = None\n",
        "\n",
        "        derivate_sigmoid = gradients*(1.0 - gradients)\n",
        "\n",
        "        return derivate_sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6360b3c0360d4193981ac91f621abef8",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "0AumYOUd2tgp",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Loss Function (Binary Cross Entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "52577f99ec9d4cb5918a774edbe97d63",
        "deepnote_cell_height": 89.390625,
        "deepnote_cell_type": "markdown",
        "id": "Z53G5Bwn2tgp",
        "tags": []
      },
      "source": [
        "$L=-\\frac{1}{N}\\sum_{i=0}^Ny_i log(p(y_i))+(1-y_i)log(1-p(y_i))$\n",
        "\n",
        "where $N$ is the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "cell_id": "f8f969c517f24b28b485149b9bdfd8c5",
        "deepnote_cell_height": 295,
        "deepnote_cell_type": "code",
        "id": "BtsbaLuL2tgq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class binary_cross_entropy():\n",
        "\n",
        "    def forward(self, output, target, batch_size=None):\n",
        "        loss = 0.0\n",
        "        \n",
        "        # TODO\n",
        "        # implement Binary Cross-Entrops loss function for output, target, batch_size selbst gemacht\n",
        "\n",
        "\n",
        "        if batch_size == None:\n",
        "            \n",
        "            if target == 1:\n",
        "\n",
        "                loss = (target*np.log(output) + (1.0 - target)*np.log(1.0-output))\n",
        "\n",
        "            else:\n",
        "\n",
        "                loss = (target*np.log(1.0-output) + (1.0 - target)*np.log(1.0-(1.0-output)))\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            if target == 1:\n",
        "\n",
        "                for i in range(batch_size):\n",
        "                    loss = loss - (1.0/batch_size)*(target*np.log(output) + (1.0 - target)*np.log(1.0-output))\n",
        "                   \n",
        "            else:\n",
        "\n",
        "                for i in range(batch_size):\n",
        "                    loss = loss - (1.0/batch_size)*(target*np.log(1.0-output) + (1.0 - target)*np.log(1.0-(1.0-output)))\n",
        "                    \n",
        "\n",
        "        # END TODO\n",
        "        return loss, output, target\n",
        "    \n",
        "    def backward(self, output, target):\n",
        "         # calculate the gradients with help of the derivative\n",
        "\n",
        "        if target == 1:\n",
        "\n",
        "            derivate_bce = (-(target/output) + (1.0 - target)/(1.0 - output))\n",
        "\n",
        "        else:\n",
        "\n",
        "            derivate_bce = (-(target/(1.0-output)) + (1.0 - target)/(1.0 - (1.0-output)))\n",
        "\n",
        "        return derivate_bce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "df9ec443c0284dc4a52c55b8caa88358",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "UQdPsvFa2tgq",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Initializing Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c251b1998be74c5fbc41cd1d47edaa88",
        "deepnote_cell_height": 101.46875,
        "deepnote_cell_type": "markdown",
        "id": "lV-m23SR2tgq",
        "tags": []
      },
      "source": [
        "Xavier intitialization is commonly used to initialize the weights of a network. It is a random uniform distribution that’s bounded between $\\pm\\frac{\\sqrt{6}}{\\sqrt{n_i+n_{i+1}}}$ where $n_i$ is the number of incoming network connections, and $n_{i+1}$ is the number of outgoing network connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "cell_id": "6b33fda27eb04ef2ad06126f77cc4dfe",
        "deepnote_cell_height": 97,
        "deepnote_cell_type": "code",
        "id": "AAONnAXQ2tgq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def xavier_initialization(n_incoming: np.ndarray, n_outgoing: np.ndarray) -> np.ndarray:\n",
        "    \"\"\" Returns a numpy array of initialized weights \"\"\"\n",
        "\n",
        "    n_incoming = np.asarray(n_incoming)\n",
        "    n_outgoing = np.asarray(n_outgoing)\n",
        "    n_in = np.sum(n_incoming)\n",
        "    n_out = np.sum(n_outgoing)\n",
        "    range = (np.sqrt(6)/np.sqrt(n_in + n_out))\n",
        "\n",
        "    xavier_initialized_weights = np.random.uniform(low = (-range), high = range, size = n_in)\n",
        "\n",
        "    return xavier_initialized_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "2cff5be7ecc843bf9056d30bf7fdb861",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "SqM-XKIr2tgq",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Implement Multiple Neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "153fa06156c54bdc8e8b10a85a586dd5",
        "deepnote_cell_height": 341.78125,
        "deepnote_cell_type": "markdown",
        "id": "oaSce1El2tgq",
        "tags": []
      },
      "source": [
        "#### Feed-Forward Layer\n",
        "\n",
        "A feed-forward layer applies a linear transformation to the input $x$ using a weight matrix $\\textbf{W}$ and a bias vector $b$:\n",
        "\n",
        "$z = x\\textbf{W}^T+b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "cell_id": "a03ea752340644e699dd3a11f8f5dc74",
        "deepnote_cell_height": 853,
        "deepnote_cell_type": "code",
        "id": "zUBUSQVq2tgr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class multi_neuron_implementation():\n",
        "    def __init__(self, number_of_neurons, loss_function, output_activation_function, bias, extra_hidden_layers):\n",
        "        self.neuron_weights = xavier_initialization((2,),(1,))\n",
        "        self.number_of_neurons = number_of_neurons\n",
        "        self.loss_function = loss_function\n",
        "        self.output_activation_function = output_activation_function\n",
        "        self.bias = bias\n",
        "        self.extra_hidden_layers = extra_hidden_layers\n",
        "        print(self.neuron_weights)\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        # TODO\n",
        "        # Implement forward propagation\n",
        "        output = 0\n",
        "\n",
        "        for i in range(self.number_of_neurons):\n",
        "\n",
        "\n",
        "            output = output + self.neuron_weights[i]*x[i]  \n",
        "            i += 1\n",
        "        output = output + self.bias\n",
        "        output = self.output_activation_function().forward(output)\n",
        "        for i in range(self.extra_hidden_layers): \n",
        "            output = self.output_activation_function().forward(output)\n",
        "        \n",
        "\n",
        "        # END TODO\n",
        "        return output, x\n",
        "\n",
        "    def backward_pass(self, x, target, input):\n",
        "        # TODO\n",
        "        # Perform backpropagation by calculating derivative\n",
        "        # input von x_0 und x_1\n",
        "        output = None\n",
        "        output = (self.loss_function().backward(x, target))*(self.output_activation_function().backward(x))\n",
        "        for i in range(self.extra_hidden_layers):\n",
        "            output = self.output_activation_function().backward(output)\n",
        "        output = output*input\n",
        "        # END TODO\n",
        "        return output\n",
        "\n",
        "    def update_parameter(self, derivative, learning_rate = 1):\n",
        "        # TODO\n",
        "        # Perform weight update\n",
        "        # use self.neuron_weights\n",
        "        new_weights = None\n",
        "        new_weights = self.neuron_weights - learning_rate*derivative\n",
        "        # END TODO\n",
        "        self.neuron_weights = new_weights\n",
        "\n",
        "    def train(self, input_data, targets):\n",
        "        # TODO\n",
        "        # Call the necessary functions to train the model with multiple neurons for the given task\n",
        "        # Complete the rest of the code to correctly train the model\n",
        "        for i in range(len(input_data)):\n",
        "            output_tr, input_tr = self.forward_pass(input_data[i])\n",
        "            derivate_tr = self.backward_pass(output_tr, targets[i], input_tr)\n",
        "            self.update_parameter(derivate_tr, learning_rate=1)      \n",
        "        # END TODO\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        # TODO\n",
        "        # Test the trained model\n",
        "        \n",
        "        output = []\n",
        "        var1 = None\n",
        "        var2 = None\n",
        "\n",
        "        \n",
        "        for i in range(len(input_data)) : \n",
        "          var1, var2 = self.forward_pass(input_data[i])       \n",
        "          output.append(var1)\n",
        "\n",
        "        output = np.asarray(output)\n",
        "\n",
        "        # END TODO\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d00fc9e07a9e4453beacaaa916ba6f75",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "bceJ5_Or2tgr",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "cell_id": "0163bbd275bd4f11a64251c9b1bee75d",
        "deepnote_cell_height": 151,
        "deepnote_cell_type": "code",
        "id": "oo-iCWs12tgr",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.81952103 -0.99499788]\n"
          ]
        }
      ],
      "source": [
        "multi_neuron = multi_neuron_implementation(number_of_neurons=2, loss_function=binary_cross_entropy, output_activation_function=sigmoid_activation_function,bias = 1, extra_hidden_layers=3)\n",
        "\n",
        "# TODO\n",
        "# train the corresponding single neuron \n",
        "multi_neuron.train(input_data, targets)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1bd4b4a7b8094b52a0c7580307ee0d92",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "Y8n4VeLH2tgr",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "cell_id": "06c36f9c0ecc42a7a71d82cf0f76a0a4",
        "deepnote_cell_height": 133,
        "deepnote_cell_type": "code",
        "id": "fnnCxdic2tgr",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.65985098 0.66205247 0.65981958 0.66204444]\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# Test the trained model\n",
        "predictions = multi_neuron.inference(input_data)\n",
        "print(predictions)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fe84c072be20440fbea2c82fa9fb57b2",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "8d6t_mIT2tgs",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "cell_id": "bf82e1a3072f47c295a5a557ca9a32e8",
        "deepnote_cell_height": 133,
        "deepnote_cell_type": "code",
        "id": "Ot4Dw5Cb2tgs",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# Call accuracy function and provide necessary inputs to calculate accuracy\n",
        "accuracy_value = accuracy(predictions, targets)\n",
        "print(accuracy_value)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ed9c69e1f9d7460b89f6899e938a68bf",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "9zwDNf042tgs",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Multi-Layer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "84f317b46e3b483fae7df63d0dc25b17",
        "deepnote_cell_height": 414.625,
        "deepnote_cell_type": "markdown",
        "id": "c0ijjfgD2tgs",
        "tags": []
      },
      "source": [
        "Let's generalize even further and build a network with an arbitrary (parametrized) number of hidden layers and hidden dimensions. For the XOR task specifically, we will consider a network with three hidden layers and a hidden dimension of three. We will also add an activiation function to introduce nonlinearity in our hidden layers.\n",
        "\n",
        "<img src=\"https://i.imgur.com/IUQ05Ol.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e731239aef1b43b6a1b58b47a03f0682",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "RJYI8_GE2tgs",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 1)\n",
            "()\n"
          ]
        }
      ],
      "source": [
        "#input data shape geandert für mlp weil (2,1) unterschiedlich von (2,)\n",
        "\n",
        "\n",
        "input_data = np.array([[[0],[0]] , [[1],[0]] , [[0],[1]] , [[1],[1]]], dtype=float)\n",
        "targets = np.array([(0), (1), (1), (0)], dtype = float)\n",
        "\n",
        "\n",
        "\n",
        "print(input_data[0].shape)\n",
        "print(targets[0].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "cell_id": "163c21039acf4345856cc2d1fec878d7",
        "deepnote_cell_height": 997,
        "deepnote_cell_type": "code",
        "id": "7kB5OBuF2tgs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from re import I\n",
        "\n",
        "\n",
        "class MLP_implementation():\n",
        "    def __init__(self,\n",
        "        input_size,\n",
        "        hidden_layers,\n",
        "        hidden_layers_size,\n",
        "        hidden_activation_func,\n",
        "        output_size,\n",
        "        output_activation_function,\n",
        "        loss_function,\n",
        "    ):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_layers_size = hidden_layers_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_activation_func = hidden_activation_func\n",
        "        self.loss_function = loss_function\n",
        "        self.output_activation_function = output_activation_function\n",
        "        #input_size und output_size selbst gemacht\n",
        "        # TODO\n",
        "        # Implement your MLP model \n",
        "\n",
        "        self.weights_list = []\n",
        "\n",
        "        for i in range(self.hidden_layers + 1):\n",
        "            if i == 0:\n",
        "                self.weights_list.append(np.random.randn(self.hidden_layers_size, self.input_size)*np.sqrt(1. / self.hidden_layers_size))\n",
        "                \n",
        "\n",
        "            elif i == self.hidden_layers:\n",
        "                self.weights_list.append(np.random.randn(self.output_size, self.hidden_layers_size)*np.sqrt(1. / self.hidden_layers_size))\n",
        "                \n",
        "            else:\n",
        "                self.weights_list.append(np.random.randn(self.hidden_layers_size, self.hidden_layers_size)*np.sqrt(1. / self.hidden_layers_size))\n",
        "                \n",
        "        # END TODO\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        # TODO\n",
        "        # Implement forward propagation\n",
        "        # Its the same principle as in the lecture \n",
        "        # I use self.hidden states for the backpro with h1 = self.forward_hidden_states[1] \n",
        "        # the first index is None (better for visualisation and if it is used by error i would get a Nan as output)\n",
        "        output = None\n",
        "        \n",
        "        self.forward_hidden_states = [None]\n",
        "       \n",
        "\n",
        "        for i in range (self.hidden_layers + 1):\n",
        "            if i == 0:\n",
        "                \n",
        "                \n",
        "                output = self.hidden_activation_func().forward(np.dot(self.weights_list[0], x))\n",
        "                \n",
        "                self.forward_hidden_states.append(output)     \n",
        "\n",
        "            elif i == self.hidden_layers:  \n",
        "\n",
        "                output_final = self.output_activation_function().forward(np.dot(self.weights_list[i], output)) \n",
        "                \n",
        "            \n",
        "            else:\n",
        "                output = self.hidden_activation_func().forward(np.dot(self.weights_list[i], output))\n",
        "\n",
        "                self.forward_hidden_states.append(output) \n",
        "                \n",
        "        \n",
        "        # END TODO\n",
        "        return output_final, x \n",
        "\n",
        "    def backward_pass(self, x, target, input_of_mlp):\n",
        "        # TODO\n",
        "        # Perform backpropagation by calculating derivative\n",
        "        # x = output von MLP\n",
        "        # Same procedure as in class with matrix multiplication. I use a list as an ouput and append from the end towards the beginning for the weights\n",
        "        # o and v are variables which will not be used again. its due to the forward return\n",
        "    \n",
        "        loss, o, v = self.loss_function().forward(x, target)\n",
        "\n",
        "        output = [None]*(self.hidden_layers+1)\n",
        "\n",
        "        \n",
        "        self.hidden_states_backprop = [None]*(self.hidden_layers+1)\n",
        "\n",
        "        # for it to be more conveniant i used the already calculed expression in the Classes dl/do = P-Y as we have a softmax and cross entropy. It needs less computational power\n",
        "        # As i already calculated the output using the loss function i did the same for the loss which is printed after each iteration. Which could be plotted as a graph during training\n",
        "        # I used the output size as we use softmax and cross entropy for muti output probalities \n",
        "    \n",
        "\n",
        "        if self.output_size > 1 :\n",
        "\n",
        "            error_at_output = x-target\n",
        "            \n",
        "            \n",
        "        else:\n",
        "            \n",
        "            error_at_output = np.dot(self.loss_function().backward(x, target), self.output_activation_function().backward(x))\n",
        "            print(error_at_output)\n",
        "\n",
        "        \n",
        "        for i in range(self.hidden_layers, -1, -1):\n",
        "\n",
        "            \n",
        "            if i == self.hidden_layers:\n",
        "                # w3 h3\n",
        "            \n",
        "                output[i] = np.dot(error_at_output ,(self.forward_hidden_states[i].T))\n",
        "                self.hidden_states_backprop[i] = np.dot(self.weights_list[i].T, error_at_output)\n",
        "                \n",
        "\n",
        "            elif i == 0:\n",
        "\n",
        "                c = self.hidden_states_backprop[i+1] * self.hidden_activation_func().backward(self.forward_hidden_states[i+1])                                                            \n",
        "\n",
        "                output[i] = np.dot( c , input_of_mlp.T)\n",
        "                   \n",
        "\n",
        "            else:\n",
        "\n",
        "                a = self.hidden_states_backprop[i+1]* self.hidden_activation_func().backward(self.forward_hidden_states[i+1])\n",
        "\n",
        "                output[i] = np.dot( a, self.forward_hidden_states[i].T)\n",
        "\n",
        "                b =   self.hidden_activation_func().backward(self.forward_hidden_states[i+1])* self.hidden_states_backprop[i+1]\n",
        "\n",
        "                self.hidden_states_backprop[i] = np.dot(self.weights_list[i].T, b )\n",
        "\n",
        "                \n",
        "\n",
        "        # END TODO\n",
        "        return output, loss\n",
        "\n",
        "    def update_parameter(self, derivative, learning_rate = 0.015, loss=None):\n",
        "        # TOD\n",
        "        # Perform weight update\n",
        "        \n",
        "        for i in range(self.hidden_layers + 1):\n",
        "            \n",
        "            \n",
        "            self.weights_list[i] = self.weights_list[i] - learning_rate*derivative[i]\n",
        "            \n",
        "        # END TODO\n",
        "\n",
        "    def train(self, input_data, targets):\n",
        "        # TODO\n",
        "        # Call the necessary functions to train the model for the given task\n",
        "        # Complete the rest of the code to correctly train the model\n",
        "\n",
        "        for i in range(len(input_data)):\n",
        "\n",
        "            output, mlp_input = self.forward_pass(input_data[i])\n",
        "            \n",
        "            weight_error, loss = self.backward_pass(output, targets[i], mlp_input)\n",
        "\n",
        "            self.update_parameter(weight_error, learning_rate=1, loss = None)\n",
        "\n",
        "            print('loss = ', loss)\n",
        "\n",
        "        # END TODO\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        # TODO\n",
        "        # Test the trained model\n",
        "\n",
        "        output = []\n",
        "        var1 = None\n",
        "        var2 = None\n",
        "\n",
        "        \n",
        "        for i in range(len(input_data)) : \n",
        "          var1, var2 = self.forward_pass(input_data[i])       \n",
        "          output.append(var1)\n",
        "\n",
        "        output = np.asarray(output)\n",
        "\n",
        "        # END TODO\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5f712a71667a48fba5caf101171168cb",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "KbVJQkvx2tgt",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Adding Nonlinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4f3d994ea8c4a7892f2c1f1f1d71082",
        "deepnote_cell_height": 651.015625,
        "deepnote_cell_type": "markdown",
        "id": "TJx_JZbl2tgt",
        "owner_user_id": "06b28ca6-80fe-4ecd-a509-50438de77bba",
        "tags": []
      },
      "source": [
        "This time, you need to implement and apply nonlinearity. For this, you should implement Rectified Linear Unit (ReLU) and apply it to provide nonlinearity to the network.\n",
        "\n",
        "Basically, ReLU activation function is a mathematical operation that processes the input data and checks whether the input is positive or not. If it is positive, then it does not change anything. Otherwise, ReLU outputs zero. \n",
        "\n",
        "When we examine the ReLU behavior, it looks like it is the combination of two different linear functions. This property makes the training easier yet effective since ReLU does not have any learnable parameters as well as easy to apply because of combination of two simple linear functions. The following equation and figure show how ReLU acts.\n",
        "\n",
        "$$ y = max(0, x) $$\n",
        "\n",
        "<center><figure><img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"450\"/><figcaption>Graph of the ReLU activation function. <a href=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\">Image is taken from</a></figcaption></figure></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "cell_id": "9ea233c84766446f966fa64794d2d63f",
        "deepnote_cell_height": 349,
        "deepnote_cell_type": "code",
        "id": "8W-5egab2tgt",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# You need to use the same implementation that you do in the previous task, if you need.\n",
        "# You only need to include ReLU activation function in your implementation.\n",
        "\n",
        "class relu_activation_function():\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement ReLU function for the input_data\n",
        "        \n",
        "        output = np.maximum(0.0,input_data) \n",
        "        \n",
        "        \n",
        "        # END TODO\n",
        "        return output\n",
        "    \n",
        "    def backward(self, gradients):\n",
        "         # calculate the gradients with help of the derivative\n",
        "        \n",
        "        gradients[gradients<=0] = 0.0\n",
        "        \n",
        "        \n",
        "\n",
        "        return gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f3eb7f63f1ef4b76907870481e5dde3e",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "0l6UYSco2tgt",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### MLP Inititialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "cell_id": "c463f6ca71c64121a27152c58878f53f",
        "deepnote_cell_height": 79,
        "deepnote_cell_type": "code",
        "id": "oUEAOGUT2tgt",
        "tags": []
      },
      "outputs": [],
      "source": [
        "relu = relu_activation_function()\n",
        "xor_mlp = MLP_implementation(2, 3, 3, relu_activation_function, 1, sigmoid_activation_function, binary_cross_entropy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "43d7dc2d8c4448f7ac6aaf1c084f695a",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "CyX7sj5Q2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "cell_id": "5f343b3459d246c8a0b8cdd5fc07a6c7",
        "deepnote_cell_height": 115,
        "deepnote_cell_type": "code",
        "id": "hoi1RVs_2tgu",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5\n",
            "loss =  -0.6931471805599453\n",
            "-0.48985186101077344\n",
            "loss =  -0.6730541268352774\n",
            "-0.4888275958166507\n",
            "loss =  -0.6710483598028536\n",
            "0.47660489388551275\n",
            "loss =  -0.64741863911229\n"
          ]
        }
      ],
      "source": [
        "# Train the same model with ReLU activation function\n",
        "# TODO\n",
        "xor_mlp.train(input_data, targets)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "be53793e58d24e8a847e7819c7c8b3ce",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "JSdR2IVW2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "cell_id": "04807c8322c24ced887c6cb5df3d7f32",
        "deepnote_cell_height": 115,
        "deepnote_cell_type": "code",
        "id": "yKDqNb5f2tgu",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.5        0.50953659 0.50988458 0.51941385]\n",
            "[0. 1. 1. 1.]\n",
            "0.75\n"
          ]
        }
      ],
      "source": [
        "# Test and evaluate your new model as in the previous task\n",
        "# TODO\n",
        "predictions = xor_mlp.inference(input_data)\n",
        "print(predictions)\n",
        "accuracy_value = accuracy(predictions, targets)\n",
        "print(predictions)\n",
        "print(accuracy_value)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "47b0a74bd12b4a3b9a395514d0b7683b",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "P1rqjyq42tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3f7b87070bb5488eb7116348ba9e5052",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "tTiNaSeM2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Iris Dataset 🌷"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8d25765564d04e78b0a65b42ba08d5b4",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "gsMh6WiW2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "Iris is a genus of hundreds of species of flowering plants with showy flowers. The Iris data set consists of 150 samples from three species of Iris which are hard to distinguish (Iris setosa, Iris virginica and Iris versicolor). There are four features from each sample: the length and the width of the sepals and petals, in centimeters. Based on these features, the goal is to predict which species of Iris the sample belongs to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "46d56be3c85c4d699ed79f2b482a31c0",
        "deepnote_cell_height": 408.875,
        "deepnote_cell_type": "markdown",
        "id": "fftFJ3ed2tgv",
        "tags": []
      },
      "source": [
        "<center><img src=\"https://www.oreilly.com/library/view/python-artificial-intelligence/9781789539462/assets/462dc4fa-fd62-4539-8599-ac80a441382c.png\" width=\"450\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d853a4f5816e49a6ac838230e0655087",
        "deepnote_cell_height": 62,
        "deepnote_cell_type": "markdown",
        "id": "lIB2XdFb2tgv",
        "tags": []
      },
      "source": [
        "###  Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "cell_id": "263a53c7e9754ea488149500a7842994",
        "deepnote_cell_height": 187,
        "deepnote_cell_type": "code",
        "id": "eMgXi4pF2tgv",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.60759494]\n",
            " [0.77272727]\n",
            " [0.27536232]\n",
            " [0.08      ]]\n",
            "[[1.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.utils import shuffle\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and test dataset\n",
        "# they are ndarray\n",
        "# we change them also in one hot encodings for the probalities, also we shuffle them so the data is better distributed\n",
        "# However it doesnt watch out about the distribution of the different classes, We could use the train test split from sklearn so the class representation is 'similar' on the train and test Set\n",
        "# But we had to use numpy and it wasnt really a task itself so i didnt looked further\n",
        "# We also normalized the data so the MLP can learn better \n",
        "\n",
        "X_shuffled, y_shuffled = shuffle(X, y)\n",
        "\n",
        "X_shuffled_normed = X_shuffled / X_shuffled.max(axis=0)\n",
        "\n",
        "X_shuffled_good_dim = [None]*(len(X_shuffled))\n",
        "\n",
        "X_shuffled_good_dim = np.array(X_shuffled_good_dim)\n",
        "\n",
        "y_one_hot_good_dim = [None]*(len(y_shuffled))\n",
        "\n",
        "y_one_hot_good_dim = np.array(y_one_hot_good_dim)\n",
        "\n",
        "y_one_hot_enc = np.eye(3)[y_shuffled.reshape(-1)]\n",
        "\n",
        "for i in range(len(X_shuffled)):\n",
        "    X_shuffled_good_dim[i] = X_shuffled_normed[i].reshape((4,1))\n",
        "\n",
        "for i in range(len(y_shuffled)):\n",
        "    y_one_hot_good_dim[i] = y_one_hot_enc[i].reshape((3,1))\n",
        "\n",
        "train_length = int(0.8*len(X))\n",
        "\n",
        "X_train, y_train = X_shuffled_good_dim[0:train_length], y_one_hot_good_dim[0:train_length]\n",
        "X_test, y_test = X_shuffled_good_dim[train_length:-1], y_one_hot_good_dim[train_length:-1]\n",
        "print(X_train[0])\n",
        "print(y_train[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c94f655f005b4452be7c097443ac0911",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "1DXogR8U2tgv",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "53272c91c1fe430e97623f79398d2da4",
        "deepnote_cell_height": 176.296875,
        "deepnote_cell_type": "markdown",
        "id": "b1d8caEY2tgv",
        "tags": []
      },
      "source": [
        "Previously, we only considered a **binary classification problem**. Iris, however, is a **multiclass classification problem** that requires us to distinguish between three classes. For this case, we can use a **Softmax activation function** in the output layer to transform our outputs (logits) to a probability distribution over our classes.\n",
        "\n",
        "Softmax is defined as\n",
        "\n",
        "$\\texttt{softmax}(z)_i=\\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.8360188 ]\n",
            " [0.11314284]\n",
            " [0.05083836]]\n"
          ]
        }
      ],
      "source": [
        "def softmax_test(x):\n",
        "    e =np.exp(x - np.max(x))\n",
        "    p = e/e.sum()\n",
        "    return p\n",
        "    \n",
        "trest = [[3.0], [1.0], [0.2]]\n",
        "print(softmax_test(trest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "cell_id": "da6150862276491195d5ef4989a74a88",
        "deepnote_cell_height": 295,
        "deepnote_cell_type": "code",
        "id": "4D1DGjVZ2tgv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class softmax_activation_function():\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement Softmax function for the input_data\n",
        "        # numerisch stabiler softmax\n",
        "       \n",
        "\n",
        "        y = np.exp(input_data - np.max(input_data))\n",
        "        output = y / y.sum()\n",
        "        \n",
        "        # END TODO\n",
        "        return output\n",
        "    \n",
        "    def backward(self, gradients):\n",
        "        # calculate the gradients with help of the derivative\n",
        "\n",
        "        jacobian_m = np.diag(gradients)\n",
        "\n",
        "        for i in range(len(jacobian_m)):\n",
        "            for j in range(len(jacobian_m)):\n",
        "                if i == j:\n",
        "                    jacobian_m[i][j] = gradients[i] * (1-gradients[i])\n",
        "                else: \n",
        "                    jacobian_m[i][j] = -gradients[i]*gradients[j]\n",
        "        return jacobian_m\n",
        "\n",
        "        #return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b30a2de548474d448eaabfd6394b62b6",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "0dpiqGij2tgv",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Loss Function (Cross-Entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "98898d7f88a745489ee0a3b8b51edc3c",
        "deepnote_cell_height": 148.171875,
        "deepnote_cell_type": "markdown",
        "id": "OjYwy0SP2tgw",
        "tags": []
      },
      "source": [
        "Related to the previous notes about sigmoid and softmax, we now also need to move from a binary cross entropy loss to a more general cross entropy loss for a multiclass classification problem.\n",
        "\n",
        "Cross-Entropy loss is defined as:\n",
        "\n",
        "$L=-\\frac{1}{N}\\sum_{n=0}^{N}\\sum_i y_i log(y_i')$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "cell_id": "b1ff232f1893454092631f79f9a073bf",
        "deepnote_cell_height": 295,
        "deepnote_cell_type": "code",
        "id": "nGpfFjG62tgw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class cross_entropy_loss():\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement Cross-Entrops loss function for output, target\n",
        "       \n",
        "        loss = np.sum(target*np.log(output))\n",
        "        \n",
        "        # END TODO\n",
        "        return loss, output, target\n",
        "    \n",
        "    def backward(self, output, target):\n",
        "        # calculate the gradients with help of the derivative\n",
        "        \n",
        "        derivate = -target*(1/output)\n",
        "\n",
        "        return derivate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "379f8681cb0f4d54bdb2a83938685764",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "TaCmfbHE2tgw",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a9ab559f86de48bd8f2ee8f13a64fbda",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "rT-aJ-3f2tgw",
        "tags": []
      },
      "source": [
        "We will again use an MLP for this task. Intitialize a model with **4 hidden layers** and a **hidden layer size of 24**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "651c8f0d9ea44ddc9ab7ad6aa7183498",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "lPRWF1BF2tgw",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "cell_id": "717328abeaf14a6a929c5a51466451b5",
        "deepnote_cell_height": 79,
        "deepnote_cell_type": "code",
        "id": "HVVl8hwC2tgw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "iris_mlp = MLP_implementation(4, 4, 24, relu_activation_function, 3, softmax_activation_function, cross_entropy_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss =  -1.094810848524672\n",
            "loss =  -1.1296956467855443\n",
            "loss =  -1.1761605018003272\n",
            "loss =  -1.1006173788689138\n",
            "loss =  -1.0542484597093058\n",
            "loss =  -0.987129511084618\n",
            "loss =  -1.208723391213119\n",
            "loss =  -1.1335378517539678\n",
            "loss =  -1.0134629463581801\n",
            "loss =  -0.9621954419428576\n",
            "loss =  -1.1213000700365126\n",
            "loss =  -1.0709877002452317\n",
            "loss =  -1.3131761040037364\n",
            "loss =  -1.0362895087494735\n",
            "loss =  -1.0372370656175236\n",
            "loss =  -1.010546118746285\n",
            "loss =  -1.3138789254806165\n",
            "loss =  -1.006413847633985\n",
            "loss =  -1.0569059935492622\n",
            "loss =  -0.9846588777218911\n",
            "loss =  -1.3242427847118123\n",
            "loss =  -1.0628418281361842\n",
            "loss =  -1.0137634919912064\n",
            "loss =  -1.006028959898533\n",
            "loss =  -0.953469950813325\n",
            "loss =  -1.3621328230033884\n",
            "loss =  -1.0561353139203045\n",
            "loss =  -0.9993689890194964\n",
            "loss =  -0.9562857721124082\n",
            "loss =  -0.9303551820878171\n",
            "loss =  -1.399924038705112\n",
            "loss =  -1.067123955245302\n",
            "loss =  -1.2933100371664439\n",
            "loss =  -0.9650366299704213\n",
            "loss =  -1.0531927046721796\n",
            "loss =  -0.9264779375212445\n",
            "loss =  -0.8893942076682116\n",
            "loss =  -1.4180949774067486\n",
            "loss =  -0.9171105571190209\n",
            "loss =  -1.11154858405624\n",
            "loss =  -0.8399517200931298\n",
            "loss =  -1.353648248097887\n",
            "loss =  -1.1149365647311837\n",
            "loss =  -1.0467987125345672\n",
            "loss =  -1.2833938383896772\n",
            "loss =  -0.9036579650741101\n",
            "loss =  -1.289511104450122\n",
            "loss =  -0.888922185242403\n",
            "loss =  -1.0700952048456516\n",
            "loss =  -1.275960105150785\n",
            "loss =  -1.232995952724917\n",
            "loss =  -1.0660253486185591\n",
            "loss =  -0.9915882417098454\n",
            "loss =  -1.0101357599274148\n",
            "loss =  -1.207409041177434\n",
            "loss =  -1.1648184401250494\n",
            "loss =  -1.0271998648173455\n",
            "loss =  -1.1640774222271952\n",
            "loss =  -1.0249753886209385\n",
            "loss =  -1.1520274385969511\n",
            "loss =  -1.1131299713708818\n",
            "loss =  -1.08738138547355\n",
            "loss =  -1.0203377257360795\n",
            "loss =  -1.0665195147798072\n",
            "loss =  -1.0418822073045824\n",
            "loss =  -0.9956086722417776\n",
            "loss =  -1.0408570447820276\n",
            "loss =  -0.9575388902942523\n",
            "loss =  -0.9990053367476018\n",
            "loss =  -0.9597304629250698\n",
            "loss =  -0.9308620892431473\n",
            "loss =  -1.0122198571354621\n",
            "loss =  -1.206727718847598\n",
            "loss =  -1.2043101009712616\n",
            "loss =  -0.9792043648129074\n",
            "loss =  -1.1471254263427275\n",
            "loss =  -1.134698770543214\n",
            "loss =  -0.9670493745256398\n",
            "loss =  -1.0103344521618325\n",
            "loss =  -1.1414402007532516\n",
            "loss =  -0.9561855253720042\n",
            "loss =  -0.9218372762774637\n",
            "loss =  -1.1329680554561592\n",
            "loss =  -1.085107912901899\n",
            "loss =  -1.0276688631332278\n",
            "loss =  -0.9561040872370329\n",
            "loss =  -0.9791398214153474\n",
            "loss =  -1.1174710301067592\n",
            "loss =  -1.078893473971741\n",
            "loss =  -1.0026762459541299\n",
            "loss =  -0.9319680644167858\n",
            "loss =  -1.0692383372869319\n",
            "loss =  -1.026540983614124\n",
            "loss =  -0.9738543976284146\n",
            "loss =  -1.006305457647585\n",
            "loss =  -1.133941760218908\n",
            "loss =  -0.9072678950366949\n",
            "loss =  -0.8417431621509687\n",
            "loss =  -1.0004498179059995\n",
            "loss =  -0.8292232482460443\n",
            "loss =  -1.2202017501039595\n",
            "loss =  -1.0264093569153026\n",
            "loss =  -1.1419489871369344\n",
            "loss =  -1.0489209909602124\n",
            "loss =  -1.0015989192074284\n",
            "loss =  -0.9436464416307556\n",
            "loss =  -0.9612779367450359\n",
            "loss =  -0.9108824233152375\n",
            "loss =  -0.8277907642427633\n",
            "loss =  -0.9399158729418154\n",
            "loss =  -0.8279324727420743\n",
            "loss =  -0.7318522943703978\n",
            "loss =  -0.7053185585118925\n",
            "loss =  -1.1646344965023643\n",
            "loss =  -1.5157687191708944\n",
            "loss =  -0.9462253607255965\n",
            "loss =  -1.2138081808442582\n",
            "loss =  -0.8874819579980282\n",
            "loss =  -0.9719144918994616\n",
            "loss =  -1.005512691098174\n"
          ]
        }
      ],
      "source": [
        "iris_mlp.train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e383e8479b24456fb456a2b163651d8e",
        "deepnote_cell_height": 62,
        "deepnote_cell_type": "markdown",
        "id": "NGxuBpxx2tgw",
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "12d63c77198848e08774450159eb4f5b",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "kRnjFbCD2tgx",
        "tags": []
      },
      "source": [
        "Show the overall accuracy of our model on the test dataset. Use the existing `accuracy` function that you implemented earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "cell_id": "f3491bbddfce45f3bbcac76872d99a7a",
        "deepnote_cell_height": 223,
        "deepnote_cell_type": "code",
        "id": "9ujmW5mT2tgx",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iris accuracy = 82.75862068965517\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# Test the trained model\n",
        "predictions_MLP = iris_mlp.inference(X_test)\n",
        "#print(predictions_MLP)\n",
        "\n",
        "# Call accuracy function and provide necessary inputs to calculate accuracy\n",
        "accuracy_value_MLP, y_final_predicted = accuracy(predictions_MLP,y_test)\n",
        "print(\"iris accuracy =\" ,accuracy_value_MLP*100)\n",
        "\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(29, 3)\n",
            "(29, 3)\n"
          ]
        }
      ],
      "source": [
        "# Reshape for the confusion matrix as we have dimensions of (29,3,1)\n",
        "\n",
        "y_bruh = np.empty((29,3,1))\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    y_bruh[i] = y_test[i]\n",
        "\n",
        "y_bruh= y_bruh.reshape((29,3))\n",
        "y_final_predicted = y_final_predicted.reshape((29,3))\n",
        "\n",
        "print(y_bruh.shape)\n",
        "print(y_final_predicted.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "157c73719d994eb1a61b10d78034ae37",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "qK4VI1br2tgx",
        "tags": []
      },
      "source": [
        "Print the confusion matrix using `sklearn.metrics.confusion_matrix`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "cell_id": "ce02ae5b924440b398b5ca899d94a6f6",
        "deepnote_cell_height": 97,
        "deepnote_cell_type": "code",
        "id": "8Tr40s4f2tgx",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[10,  0,  0],\n",
              "       [ 0,  9,  0],\n",
              "       [ 0,  5,  5]], dtype=int64)"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_bruh.argmax(axis=1), y_final_predicted.argmax(axis=1),labels=[0,1,2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cd41f0bd4689449b85001f94469b6dfb",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "9gtdQkiC2tgx",
        "tags": []
      },
      "source": [
        "Now please also look at the confusion matrix, what can you conclude from it? (no code, write text as part of this question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9bece33cdd404cd3bbf2c11a7ea557d5",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "9gykcH6x2tgx",
        "tags": []
      },
      "source": [
        "We can see that from the 29 test samples 24 are predicted correctly. Only 5 are not classified correctly however they are all classified as the same class(five virginica are classified as versicolors). This could be due to the order of training dataset as it gets updated after each sample. The solution could be to have good mix/order of the different classes for the test set. But we have an overall accuracy of 82.75% which can be improved either by twisting the hyperparameters or by having a more even data/classes distribution in the training and test dataset. After each run we will have a different data distribution due to the shuffle operation and also different weights due to random. Also we could have better accuracys (in general) and by implementing a bias in our MLP for example in the mlp xor task. Our accuracy could also be more Stable if we used batch size Training instead of an update for each sample which makes it highly dependet of the order of training dataset. However the update for each sample is better for finetuning our model and get better general accuracys for a given Problem. The best we had was 96%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "-E6ZLcXm2tgx",
        "tags": []
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=faa4af3b-d086-4f42-8b7d-d29c91b1d0f6' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QW5c6tT52tgl",
        "oCy6z6Ig2tgm",
        "5kpPuTNY2tgn",
        "TD9gtzyL2tgn",
        "DcMjb5zH2tgo",
        "jc6g4n0-2tgp",
        "NHAGXyrR2tgp",
        "0AumYOUd2tgp",
        "UQdPsvFa2tgq",
        "SqM-XKIr2tgq",
        "oaSce1El2tgq",
        "bceJ5_Or2tgr",
        "8d6t_mIT2tgs",
        "RJYI8_GE2tgs",
        "KbVJQkvx2tgt",
        "0l6UYSco2tgt",
        "CyX7sj5Q2tgu",
        "JSdR2IVW2tgu",
        "tTiNaSeM2tgu",
        "lIB2XdFb2tgv",
        "1DXogR8U2tgv",
        "0dpiqGij2tgv",
        "TaCmfbHE2tgw",
        "lPRWF1BF2tgw",
        "NGxuBpxx2tgw"
      ],
      "name": "exercise1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "50c68a69-c321-4e24-b142-cdaa47048c60",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.11 ('lama_gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "76b5a6a0fd23a51652c72cb0a227f6fd461941cc2403fd7df3d444978557e71e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
