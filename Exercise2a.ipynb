{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biggojo/Assignments-DL/blob/main/Exercise2a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This submission is for:\n",
        "- Abraham Gassama (2285843)\n",
        "- Mikail Eroglu (2281738)\n",
        "- Zhuo Le Lee (2317240)"
      ],
      "metadata": {
        "cell_id": "e6929dd6290a45bbbeba2dbd64adda8b",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 153.171875,
        "id": "fW922_J_pOO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2A - Transformers"
      ],
      "metadata": {
        "cell_id": "9339153b4e8f400894090f4cdec8212f",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 82,
        "id": "xPQt80WhpOPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you'll implement a basic encoder-only Transformer architecture with PyTorch. We will start with building the basic building blocks and then integrate them into a fully-fleged Transformer model. We train the model to solve a POS-Tagging problem (more on that later). In the previous exercise, you implemented your work in numpy. Now, we will switch to PyTorch, which will track the gradients for us and allows us to focus more on the network itself."
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "eef422cc65524297ab7b1220162873b0",
        "tags": [],
        "is_collapsed": false,
        "owner_user_id": "175a6e67-5f66-4c81-960a-2a75fbd3d9af",
        "deepnote_cell_type": "text-cell-p",
        "id": "YDMaj5jWpOPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can receive up to three points for your implementation of Exercise 2A. Together with Exercise 1, you can get up to six bonus points for the exam."
      ],
      "metadata": {
        "cell_id": "64bcb0b3c2bb43a6b1620d47ecea1de6",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "m6cSfTtzpOPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Notice**: Throughout the notebook, basic structures are provided such as functions and classes without bodies or partial bodies, and variables that you need to assign to. **Don't change the names of functions, variables, and classes - and make sure that you are using them!** You're allowed to introduce helper variables and functions. Occasionally, we use **type annotations** that you should follow. They are not enforced by Python. Whenenver you see an ellipsis `...` you're supposed to insert code."
      ],
      "metadata": {
        "cell_id": "c66a0519697243859f4aa30b0a0e06c0",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 119.5625,
        "id": "-nTKihOkpOPy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "1ed55a8f3a8d4f8ea7f58929e119f5ed",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "aef22c3f",
        "execution_start": 1657123526340,
        "execution_millis": 40884,
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 696,
        "id": "o3VfJAIepOP0",
        "outputId": "ffa1dde2-168f-49f1-b991-c3d20dfca07f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torchtext torchdata torchmetrics"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchtext) (4.1.1)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.5.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "20b2dbfb9550468d85d409a5a67f64c0",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "2c538688",
        "execution_start": 1657123567236,
        "execution_millis": 810,
        "owner_user_id": "06b28ca6-80fe-4ecd-a509-50438de77bba",
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 149.375,
        "id": "0mY3MFbfpOQQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn, Tensor"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's actually start with a few basic functions that we will need throughout the exercise, namely **Softmax** and **ReLu**.\n",
        "\n",
        "$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n",
        "\n",
        "$\\text{ReLU}(x) = \\max(0, x)$"
      ],
      "metadata": {
        "cell_id": "925794d055334d71a1874bd0928f2358",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 131.1875,
        "id": "xqp7JL3PpOQT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "89cb4d8c8ca04395a96eb2deedea1989",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 133,
        "id": "1ZlGwRNGpOQW"
      },
      "source": [
        "def softmax(input: Tensor) -> Tensor:\n",
        "    \n",
        "    # Clone the input tensor and save as output tensor to be overwritten \n",
        "    output_tensor = torch.clone(input)\n",
        "    \n",
        "    # Note that this implementation assumes that the input tensor is a one directional array \n",
        "    # For a multidimensional input, the dimension along which the input elements are to be normalized should be given as an input parameter\n",
        "    exp_input = torch.exp(input)\n",
        "    sum_exp = torch.sum(exp_input)\n",
        "    output_tensor = exp_input / sum_exp\n",
        "    \n",
        "    return output_tensor\n",
        "\n",
        "def relu(input: Tensor) -> Tensor:\n",
        "    return torch.maximum(input, torch.zeros(input.size()))\n",
        "    \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing out certain pytorch functions\n",
        "'''\n",
        "sampleTensor = Tensor([10,8,2,3,5,8,10])\n",
        "print(sampleTensor[0])                                                            # Tensor can be indexed and sliced exactly like arrays\n",
        "print(sampleTensor.shape[0])\n",
        "\n",
        "sampleTensorB = torch.clone(sampleTensor)                                         # Replicating tensors -> Memory inefficient, room for improvement\n",
        "print(sampleTensorB)\n",
        "\n",
        "TwoD_tensor = Tensor([[1,2,3],[4,5,6]])\n",
        "sum_over_row = torch.sum(TwoD_tensor, dim=0)\n",
        "print(sum_over_row)\n",
        "\n",
        "exp_sampleTensor = torch.exp(sampleTensor)\n",
        "print(exp_sampleTensor)\n",
        "\n",
        "two_mul = Tensor([2,4,6,8,10,12,14,16,18])\n",
        "one_mul = two_mul / 2\n",
        "print(one_mul)\n",
        "\n",
        "print(TwoD_tensor.size())\n",
        "ones = torch.ones(TwoD_tensor.size())\n",
        "print(ones)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "KRGT3H1YZbnR",
        "outputId": "2a848ca4-d50c-4213-c758-02beb8dfe4ee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nsampleTensor = Tensor([10,8,2,3,5,8,10])\\nprint(sampleTensor[0])                                                            # Tensor can be indexed and sliced exactly like arrays\\nprint(sampleTensor.shape[0])\\n\\nsampleTensorB = torch.clone(sampleTensor)                                         # Replicating tensors -> Memory inefficient, room for improvement\\nprint(sampleTensorB)\\n\\nTwoD_tensor = Tensor([[1,2,3],[4,5,6]])\\nsum_over_row = torch.sum(TwoD_tensor, dim=0)\\nprint(sum_over_row)\\n\\nexp_sampleTensor = torch.exp(sampleTensor)\\nprint(exp_sampleTensor)\\n\\ntwo_mul = Tensor([2,4,6,8,10,12,14,16,18])\\none_mul = two_mul / 2\\nprint(one_mul)\\n\\nprint(TwoD_tensor.size())\\nones = torch.ones(TwoD_tensor.size())\\nprint(ones)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity checks for the ReLU and softmax function\n",
        "\n",
        "# Create a tensor of random values with 15 columns and 1 row to match the assumption made\n",
        "sample_input = torch.rand(15)\n",
        "\n",
        "# Note: The output tensors are rounded to 4 decimals places to allow for easier comparison\n",
        "# Output from our implementation\n",
        "SM_out1 = torch.round(softmax(sample_input), decimals=4)\n",
        "ReLU_out1 = torch.round(relu(sample_input), decimals=4)\n",
        "\n",
        "# Output from PyTorch Implementation\n",
        "SM_pytorch = nn.Softmax(dim=0)\n",
        "ReLU_pytorch = nn.ReLU()\n",
        "SM_out2 = torch.round(SM_pytorch(sample_input), decimals=4)\n",
        "ReLU_out2 = torch.round(ReLU_pytorch(sample_input), decimals=4)\n",
        "\n",
        "\n",
        "softmax_compare = torch.eq(SM_out1, SM_out2)\n",
        "relu_compare = torch.eq(ReLU_out1, ReLU_out2)\n",
        "\n",
        "\n",
        "print(softmax_compare)\n",
        "print(relu_compare)\n",
        "\n",
        "# Both functions passed the sanity checks "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLAXx1DRnW8Q",
        "outputId": "5bde3e53-b713-442d-e9ff-9a9041db1b25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "        True, True, True])\n",
            "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "        True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Block"
      ],
      "metadata": {
        "cell_id": "65d60a38ea0d41ca934e8a0f782b1672",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 70,
        "id": "_R6L7FsMpOQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A typical transformer block consists of the following \n",
        "- Multi-Head Attention\n",
        "- Layer Normalization\n",
        "- Linear Layer\n",
        "- Residual Connections"
      ],
      "metadata": {
        "cell_id": "090be921347b4f65a3dd0fca98b29c54",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 178.953125,
        "id": "tGlQFefdpOQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.imgur.com/ZKgcoe4.png\" alt=\"transformer block visualization\" width=\"200\">"
      ],
      "metadata": {
        "cell_id": "f87bf87fea1a44bcbc2d5c044843e007",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 353,
        "id": "E4l0bYkUpOQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next few subsections, we will build these basic building blocks."
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "1c8bd5f72db0415bbca4cdd48b0ecd52",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-p",
        "id": "Xm8WA_q-pOQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention"
      ],
      "metadata": {
        "cell_id": "59b1b38243a84b5c9571ddb814cd1a49",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "kD74-tq4pOQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head Attention concatenates the outputs of several so called **attention heads**.\n",
        "\n",
        "$\\textrm{MHA}(Q,K,V) = \\textrm{Concat}(H_1,...,H_h)$"
      ],
      "metadata": {
        "cell_id": "a2a24844affe40a89f26ef6ca0346276",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 88.78125,
        "id": "HUWCLswzpOQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=300>"
      ],
      "metadata": {
        "cell_id": "ef2e9b876ee44b69a993f15856b206e9",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 376.15625,
        "id": "toF2B-c3pOQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One attention head consists of linear projections for each of $Q, K$ and $V$ and an attention mechanism called **Scaled Dot-Product Attention**. The attention mechanism scales down the dot products by $\\sqrt{d_k}$.\n",
        "\n",
        "$\\textrm{Attention}(Q,K,V)=\\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
        "\n",
        "\n",
        "\n",
        "If we assume that $q$ and $v$ are $d_k$-dimensional vectors and its components are independent random variables with mean $0$ and a variance of $d_k$, then their dot product has a mean of $0$ and variance of $d_k$. It is preferred to have a variance of $1$ and that's why we scale them down by $\\sqrt{d_k}$.\n",
        "\n",
        "The dot product $q \\cdot v$ resembles a measure of similarity.\n"
      ],
      "metadata": {
        "cell_id": "a045d2caec3e4020a7bb3fee5680442b",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 233.90625,
        "id": "QUHITGgupOQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"350\">"
      ],
      "metadata": {
        "cell_id": "9495e574d00e4d3798368384c6b3825b",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 421.609375,
        "id": "4BoPKh5TpOQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start implementing these components. Note that our classes inherit from PyTorch's `nn.Module`. These modules allow us to hold our parameters and easily move them to the GPU (with `.to(...)`). It also let's us define the computation that is performed at every call, in the `forward()` method. For example, when we have an `Attention` module, initialize it like `attention = Attention(...)`, we are able to call it with `attention(Q, K, V)` (it'll execute the `forward` function in an optimized way)."
      ],
      "metadata": {
        "cell_id": "e3ed8097791f458a85f3aabfd0756d28",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 119.5625,
        "id": "KDf7KvfupOQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assumptions: \n",
        "\n",
        "\n",
        "1.   Q, K and V are **batches** of matrices, each with shape *(batch_size, seq_length, num_features)*. \n",
        "2.   The attention here is self-attention, this means that the sequence length of Q and K are the same. \n",
        "3.   The number of hidden dimensions for all Q, K and V are the same.\n",
        "4.   For added simplicity, we first assume that the batch size is one. We then introduce batched attention matrix after the simple attention has passed all of the sanity checks\n",
        "\n",
        "### Operations in Attention and their outputs: \n",
        "\n",
        "\n",
        "\n",
        "1.   Multiplying the query (Q) and key (K) arrays in a linear matrix multiplication. This is the attention of this layer and determines how important each element in the key sequences is with regard to the query sequence. \n",
        "\n",
        "> Output 1: *(batch_size, seq_length, seq_length)*\n",
        "\n",
        "2.   Scaling the attention to have variance of 1, regardless of the size of seq_length\n",
        "\n",
        "> Output 2: *(batch_size, seq_length, seq_length)*\n",
        "\n",
        "3.   Normalizing the attention array across the keys dimension with softmax, so that all the attention weights sum to one. (Because we can't pay more than 100% attention!)\n",
        "\n",
        "> Output 3: *(batch_size, seq_length, seq_length)*\n",
        "\n",
        "4.   Multiplying the attention matrix with the value (V) array using matrix multiplication. \n",
        "\n",
        "> Output 4: *(batch_size, seq_length, num_features)*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7569pcqgQRC9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "2fc20cdd2c4c4e95aa1cb044b1258d63",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 187,
        "id": "-jjFTztxpOQs"
      },
      "source": [
        "# Implementation of a single attention head\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_n: int):\n",
        "        \"\"\"\n",
        "        hidden_n : Number of hidden dimensions\n",
        "        It is here assumed that the number of hidden dimensions corresponds\n",
        "        to the number of features in each input timestep.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_n = hidden_n\n",
        "        \n",
        "        \"\"\"\n",
        "        nn.Linear intializes and defines a transformation matrix, where all but \n",
        "        the last dimension of the output is different than the input \n",
        "        \"\"\"\n",
        "        self.Q_linear = nn.Linear(hidden_n, hidden_n)\n",
        "        self.V_linear = nn.Linear(hidden_n, hidden_n)\n",
        "        self.K_linear = nn.Linear(hidden_n, hidden_n)\n",
        "\n",
        "\n",
        "    def forward(self, Q: Tensor, K: Tensor, V: Tensor, mask=None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Note 1: If it was a batched attention, then all of the following matmul \n",
        "        has to be changed to bmm (batched matrix multiplication, supported by\n",
        "        pytorch)\n",
        "\n",
        "        Note 2: The above softmax function would not work here, if the number of \n",
        "        time steps (i.e. sequence length) and the number of features are both\n",
        "        more than one. The above softmax function would have to be revised to \n",
        "        include the number of dimensions.\n",
        "\n",
        "        Note 3: The dimensions of the transpose has to be double-checked against\n",
        "        the size of the inputs to the attention layer later, when the inputs\n",
        "        are given.\n",
        "\n",
        "        Note 4: A naive implementation of the masking operation is given here, \n",
        "        should the a mask be defined as an input. This might have to be revised\n",
        "        in later steps.\n",
        "        \"\"\"\n",
        "\n",
        "        # This step can be made redundant later, when the output dimensions \n",
        "        # matches that of expected.\n",
        "        Q_arr = self.Q_linear(Q)\n",
        "        K_arr = self.K_linear(K)\n",
        "        V_arr = self.V_linear(V)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-1,1)) / (self.hidden_n ** 0.5)\n",
        "        \n",
        "        if mask is not None:\n",
        "          mask = mask.unsqueeze(1)\n",
        "          scores = scores.masked_fill(mask == 0, -1e9)\n",
        "          scores = softmax(scores)\n",
        "        \n",
        "        scores = softmax(scores)\n",
        "\n",
        "        output = torch.matmul(scores, V)\n",
        "        return output \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "4ad1acddce234ba98f163b58e34c04d1",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 241,
        "id": "fdfZ-iMSpOQt"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_n: int, h: int = 2):\n",
        "        \"\"\"\n",
        "        hidden_n: hidden dimension\n",
        "        h: number of heads\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_n = hidden_n\n",
        "        self.h = h\n",
        "        # self.d_k = hidden_n // h\n",
        "\n",
        "        \"\"\"\n",
        "        Intialize an attention head for each element in h\n",
        "        \"\"\"\n",
        "        self.heads = nn.ModuleList(\n",
        "            [Attention(hidden_n) for x in range(h)]\n",
        "        )\n",
        "        \n",
        "        \"\"\"\n",
        "        After concatenating, the output size would be (batch_size, seq_length, num_features, h)\n",
        "        This can be reverted back to the original output size via a linear layer\n",
        "\n",
        "        Note 1: There are other implementation variations regarding this step.\n",
        "        E.g. we could split the input into n heads, so that each will have the dimensions\n",
        "        (batch_size, seq_length, num_features, num_features / n), with the last dimension denoting\n",
        "        which head each input is subjected to. Each input is then passed through the according\n",
        "        attention head and then concatenated. \n",
        "\n",
        "        This variation was not chosen for simplicity, e.g. self.d_k has to be a factor of \n",
        "        self.hidden_n\n",
        "        \"\"\"\n",
        "        self.out = nn.Linear(h * hidden_n, hidden_n)\n",
        "\n",
        "   \n",
        "    def forward(self, Q: Tensor, K: Tensor, V: Tensor, mask=None):\n",
        "        return self.out(\n",
        "            torch.cat([h(Q,K,V) for h in self.heads], dim=-1)\n",
        "        )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalization"
      ],
      "metadata": {
        "cell_id": "96fa5effdd954c498f1efa8cd0580bc3",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "yUZX_55zpOQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the lecture, remember layer normalization where the values are normalized across the feature dimension, independently for each sample in the batch. For that, first calculate mean and standard-deviation across the feature dimension and then scale them appropriately such that the mean is 0 and the standard deviation is 1. Introduce **two sets of learnable parameters**, one for shifting the mean (addition) and one for scaling the variance (multiplication) the normalized features (i.e., two parameters for each feature). Tip: Use `nn.Parameter` for that."
      ],
      "metadata": {
        "cell_id": "fba0740f863c4787aa23494388944b49",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 141.953125,
        "id": "_Ufh5KKxpOQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$y_{\\textrm{norm}}=\\frac{x-\\mu}{\\sqrt{\\sigma+\\epsilon}}$\n",
        "\n",
        "$y=y_{\\textrm{norm}}\\cdot\\beta+\\alpha$"
      ],
      "metadata": {
        "cell_id": "5b0e5f8055a945929dd74b990ecd2a89",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 91.5,
        "id": "7emhwjKJpOQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://i.stack.imgur.com/E3104.png\" alt=\"visualization of layer norm vs. batch norm\" width=\"420\">"
      ],
      "metadata": {
        "cell_id": "f96e811cce6d457f85eead1edae4692f",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 92.390625,
        "id": "DYLNkoNNpOQx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "6d5af9330ebd43c5be334c4f3922031c",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9c4ce5c0",
        "execution_start": 1656077679263,
        "execution_millis": 142,
        "output_cleared": true,
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 346,
        "id": "oLOucdf_pOQx"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, norm_shape):\n",
        "        \"\"\"\n",
        "        See also: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n",
        "        \n",
        "        :param norm_shape: The dimension of the layer to be normalized, i.e. the \n",
        "        shape of the input tensor or the last dimension of the input tensor.\n",
        "\n",
        "        Note 1: It is assummed here that norm_shape is the \n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm_shape = norm_shape\n",
        "\n",
        "        \"\"\"\n",
        "        :param alpha: Add a scale parameter if it is True.\n",
        "        :param beta: Add an offset parameter if it is True.\n",
        "        :param epsilon: Epsilon for calculating variance.\n",
        "\n",
        "        The initialized value of alpha and beta should be the same as the dimension\n",
        "        of the layer to be normalized, assuming that the multiplication between\n",
        "        beta and y_norm is element-wise.\n",
        "\n",
        "        Note 1: It is often said that the parameters initialized with nn.Parameter()\n",
        "        often have dimunitive values like 1.4013e-45. Other alternatives such as\n",
        "        nn.Linear() might be worth considering, since it does initial processing to the \n",
        "        input tensor such as uniformization. \n",
        "\n",
        "        Note 2: Here, the weights are intialized to be random variables form a \n",
        "        uniform distribution on the interval [0,1)\n",
        "        \n",
        "        Note 3: The paramters can also be intialized with torch.Tensor(*norm_shape)\n",
        "        or torch.empty(norm_shape)\n",
        "        \"\"\"\n",
        "        self.alpha = torch.nn.Parameter(torch.rand(norm_shape))\n",
        "        self.beta = torch.nn.Parameter(torch.rand(norm_shape))\n",
        "        self.epsilon = 1e-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Note 1: It is assumed that the feature dimension is the last dimension, which \n",
        "        matches our assumption from above.\n",
        "        \"\"\"\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
        "        std = (var + self.epsilon).sqrt()\n",
        "        y_norm = (x - mean) / std\n",
        "\n",
        "        y = (y_norm * self.beta) + self.alpha\n",
        "        return y"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Block"
      ],
      "metadata": {
        "cell_id": "246bb7a8b64f4596b26778aa9ce5bc85",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "8M3jDuK4pOQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we bring all ingredients together into a single module. Don't forget to add the residual connections."
      ],
      "metadata": {
        "cell_id": "93c32b55767c46049ffb13bc012bb500",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "mdAHryyXpOQz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "4e040c8ebd5d42ddb8b00090dffdd960",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 241,
        "id": "GXb1LEVzpOQ0"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_n: int, h: int = 2):\n",
        "        \"\"\"\n",
        "        hidden_n: hidden dimension\n",
        "        h: number of heads\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_n = hidden_n\n",
        "        self.h = h\n",
        "        self.attention = MultiHeadAttention(hidden_n, h)\n",
        "        self.norm1 = LayerNorm(hidden_n)\n",
        "        self.norm2 = LayerNorm(hidden_n)\n",
        "\n",
        "        # This following variable can be eventually defined via input \n",
        "        ff_dim = 2048\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(hidden_n, ff_dim*hidden_n),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim*hidden_n, hidden_n)\n",
        "        )\n",
        "\n",
        "    # Assuming that the query, key and value arrays are the same and given as input\n",
        "    def forward(self, x: Tensor):\n",
        "        \"\"\"\n",
        "        Alternatively, the inputs could be normalized first before using them as\n",
        "        input for the attention layer, while the residual connections are not normalised\n",
        "\n",
        "        x_norm1 = self.norm1(x)\n",
        "        x_out1 = x + self.attention(x_norm1, x_norm1, x_norm1)\n",
        "        x_norm2 = self.norm_2(x_out1)\n",
        "        x_out2 = x_out1 + self.feed_forward(x_norm2)\n",
        "\n",
        "        return x_out2\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        x_out1 = x + self.attention(x, x, x)\n",
        "        x_attn = self.norm1(x)\n",
        "\n",
        "        x_out2 = x_attn + self.feed_forward(x_attn)\n",
        "        x_ff = self.norm2(x_out2)\n",
        "        return x_ff\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple Transformer Architecture"
      ],
      "metadata": {
        "cell_id": "add8b0cdf6ef4a79bf2a71788029d617",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 70,
        "id": "V9e1Z3q6pOQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's stack our transformer blocks and add an embedding layer for a simple transformer architecture. You are allowed to use `nn.Embedding` here."
      ],
      "metadata": {
        "cell_id": "0215525d6258493485ff71f3496128a2",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "K12QDX7wpOQ-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "613b3d3f1e7145ba8ea535c0a34a66a5",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 277,
        "id": "yi8_268IpOQ_"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, emb_n: int, hidden_n: int, n:int =3, h:int =2):\n",
        "        \"\"\"\n",
        "        emb_n: number of token embeddings\n",
        "        hidden_n: hidden dimension\n",
        "        n: number of layers\n",
        "        h: number of heads per layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def forward(self):\n",
        "        ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS-Tagging"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "24ce63a017054111a0e7606078f244eb",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h2",
        "id": "SxwJIhL9pORA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-Of-Speech-Tagging (**POS-Tagging**) is a **sequence labeling problem** where we categorize words in a text in correspondence with a particular part of speech (e.g., \"noun\" or \"adjective\"). A few examples and classes are shown in the following table:"
      ],
      "metadata": {
        "cell_id": "46e6832e31434597932051c2d7d833db",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "OAdzksGepORB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|  POS Tag  |  Description  |  Examples  |\n",
        "|-----------|------------|------------|\n",
        "|  NN | Noun (singular, common) | mass, wind, ...  |\n",
        "|  NNP | Noun (singular, proper) | Obama, Liverpool, ...  |\n",
        "| CD  | Numeral (cardinal)  | 1890, 0.5, ...  |\n",
        "|  DT | Determiner  | all, any, ... |\n",
        "| JJ | Adjective (ordinal) | oiled, third, ... |\n",
        "... many more"
      ],
      "metadata": {
        "cell_id": "0b933b07b931423dae80af31abe46b69",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 200.734375,
        "id": "lQtj6490pORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoNLL2000 Dataset"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c1ea426748d34a2891270fb4c9189387",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "GEVJAksHpORD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load our dataset which is the **CoNLL2000 dataset** and look at an example."
      ],
      "metadata": {
        "cell_id": "ec19498428f042a09867d13bfcbbeaf1",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "Fccv3ZmjpORF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "56f8d97328eb45808c642c232c18ddeb",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c7ccc53b",
        "execution_start": 1657123575378,
        "execution_millis": 3449,
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 329.375,
        "id": "D0jZVbb4pORG",
        "outputId": "c5d1f7c0-1382-48f3-8213-6d19c41dba17"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.datasets import CoNLL2000Chunking\n",
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(CoNLL2000Chunking()[0], columns=['words', 'pos_tags', 'chunk'])\n",
        "test_df = pd.DataFrame(CoNLL2000Chunking()[1], columns=['words', 'pos_tags', 'chunk'])\n",
        "\n",
        "train_src, train_tgt = train_df['words'].tolist(), train_df['pos_tags'].tolist()\n",
        "test_src, test_tgt = test_df['words'].tolist(), test_df['pos_tags'].tolist()\n",
        "\n",
        "print(train_src[0])\n",
        "print(train_tgt[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['Confidence', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'September', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'July', 'and', 'August', \"'s\", 'near-record', 'deficits', '.']\n['NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'IN', 'NNP', ',', 'JJ', 'IN', 'NN', 'NN', ',', 'VB', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'POS', 'JJ', 'NNS', '.']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to create a vocabulary. Our dataset is already tokenized. However, we need to assign ids to them in order to input them to the embedding layer. We also need the number of embeddings (`num_embeddings`) for the size of our lookup table of `nn.Embedding`.\n",
        "\n",
        "Thus, we will iterate over all sentences replace them with ids and the mapping to our vocabulary. It'll be handy to have two different mappings, from id to token, as well as, from token to id. Note that we will add a special token `<unk>` with id `0` for words that are unknown (that are not in the training dataset but could possibly be in the test dataset)."
      ],
      "metadata": {
        "cell_id": "555caa09a6714a63a9cdee39264291f7",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 178.34375,
        "id": "fvO0ZgRjpORH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "9b665796e64d4aa694d1718b5156294d",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "2276b8b9",
        "execution_start": 1656084179768,
        "execution_millis": 1512,
        "output_cleared": true,
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 148,
        "id": "eSzrCN_YpORJ"
      },
      "source": [
        "vocabulary_id2token : dict = {0: '<unk>'}\n",
        "vocabulary_token2id : dict = {'<unk>': 0}\n",
        "\n",
        "for sentence in train_src:\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the same for our classes:"
      ],
      "metadata": {
        "cell_id": "eed144592b0a4aa9a752fab5230b27e6",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "8Aj_VDDDpORM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "a4401fdd42024ef9b0af6c95af4ff963",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 133,
        "id": "YSr0BudxpORM"
      },
      "source": [
        "classes_id2name : dict = {}\n",
        "classes_name2id : dict = {}\n",
        "\n",
        "for sentence in *[train_src, test_src]:\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's use PyTorch's `Dataset` and `DataLoader` for help us batching our data. Let's also replace tokens and classes with our ids. For that, complete `get_token_ids` and `get_class_ids`."
      ],
      "metadata": {
        "cell_id": "ef26b718c8654c7cb211c7542430ee79",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "51EuviJDpORO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "33964355330b45ad98f64b88799e8672",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a6bcb220",
        "execution_start": 1656333952181,
        "execution_millis": 2,
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 508,
        "id": "1bHCQHSHpORP"
      },
      "source": [
        "def get_token_ids(src: list[str]) -> list[int]:\n",
        "    ...\n",
        "\n",
        "def get_class_ids(tgt: list[str]) -> list[int]:\n",
        "    ...\n",
        "\n",
        "class ConllDataset(Dataset):\n",
        "  def __init__(self, src, tgt):\n",
        "        self.src = src\n",
        "        self.tgt = tgt\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        src = self.src[index]\n",
        "        tgt = self.tgt[index]\n",
        "        \n",
        "        return {\n",
        "            'src': get_token_ids(src),\n",
        "            'tgt': get_class_ids(tgt),\n",
        "        }\n",
        "\n",
        "train_dataset = ConllDataset(train_src, train_tgt)\n",
        "test_dataset = ConllDataset(test_src, test_tgt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a **batch size of 32**."
      ],
      "metadata": {
        "cell_id": "ce970c07a9ca4b6ab5a4ee941f3492b9",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "fsxtB4-ApORQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "e66f66e38e4343a8ac9237e4e9bf74e5",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f1c68216",
        "execution_start": 1656086903728,
        "execution_millis": 49,
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 76,
        "id": "9vUIXD-mpORa"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, since our examples are of different length, we need to pad shorter examples to the length of the example with the maximum length in our batch. So, let's define a special **padding token** in our vocabulary:"
      ],
      "metadata": {
        "cell_id": "a4c1832e408347d89d304e7870522878",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "qnUZjljzpORb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00825cb3c6194a91b8cc03eee1a61e7c",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 115,
        "id": "8kGeA4IjpORc"
      },
      "source": [
        "padding_token = ...\n",
        "\n",
        "vocabulary_id2token ...\n",
        "vocabulary_token2id ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `collate_fn` is the function that actually receives a batch and needs to add the padding tokens, then returns `src` and `tgt` as `Tensor`s of size `[B, S]` where `B` is our batch size and `S` our maximum sequence length. This function should additionally return a `mask`, a `Tensor` with binary values to indicate whether the specific element is a padding token or not (0 if it's a padding token, 1 if not), such that we can ignore padding tokens in our attention mechanism and loss calculation. "
      ],
      "metadata": {
        "cell_id": "c7b65a2d6b654c8bbe59ad7432758a8a",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 119.5625,
        "id": "CswtGa6lpORe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "9266354695a646ebbd776e4290baa002",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d3107fb6",
        "execution_start": 1656086903777,
        "execution_millis": 0,
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 238,
        "id": "z1Bfar5ApORf"
      },
      "source": [
        "def collate_fn(batch: list[dict]) -> dict[str, Tensor]:\n",
        "    \"\"\"\n",
        "    batch: list of dictionaries with keys src and tgt (as defined in ConllDataset)\n",
        "    \"\"\"\n",
        "    ...\n",
        "    return {\n",
        "        'src': src,\n",
        "        'tgt': tgt,\n",
        "        'mask': mask,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that, we can use PyTorch's `DataLoader` which will shuffle and batch our data automatically."
      ],
      "metadata": {
        "cell_id": "27c262f705004bac83e8306a99ba8bcb",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "knrTyfFOpORh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "110b64825faf4982943ed9eff63ed0ff",
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "50d4e2b2",
        "execution_start": 1656086903778,
        "execution_millis": 0,
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 94,
        "id": "iNloZCbKpORi"
      },
      "source": [
        "train_data_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_data_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "803ef4d85f0a4e2fbcb2ee182520c7f5",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "TPzC618spORj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a transformer model with three layers, three attention heads and an embedding dimension of 128. Also, let's not forget to add a classification head to our model."
      ],
      "metadata": {
        "cell_id": "06f55de40ea248099cbf94bacf9b1268",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "BXp6bahApORk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "5f1f177f8cf343f1bd763f05a8c8703d",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 223,
        "id": "dJ8QzN22pORl"
      },
      "source": [
        "class CoNLL2000Transformer:\n",
        "    def __init__(self, transformer, ...):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        self.classification_layer = ...\n",
        "\n",
        "    def forward(self):\n",
        "        ...\n",
        "\n",
        "model = CoNLL2000Transformer(Transformer(...), ...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "cell_id": "f5aafb3762974b07be3a8a89899db999",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "UkLzvXY4pORo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the **AdamW** optimizer from the `torch.optim` module and choose the most appropriate loss function for our task."
      ],
      "metadata": {
        "cell_id": "965bfa0ea57d4052aabfdba6904f65de",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "7e8aN7JspORp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "5101fa497d074916b870e1e4fe9147d4",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 79,
        "id": "8SggjZW9pORq"
      },
      "source": [
        "optimizer = ....\n",
        "criterion = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a basic training loop and train the network for three epochs.\n",
        "- Use everything we've built to far, including `train_data_loader`, `model`, `optimizer` and `criterion`.\n",
        "- At every 50th step print the average loss of the last 50 steps. \n",
        "- It is suggested to make a basic training procedure to work on the CPU first. Once it successfully runs on the CPU, you can switch to the GPU (click on change runtime and add an hardware accelerator if you use Colab) and run for the whole three epochs. Note: For this to work, you need to transfer the `model` and the input tensors to the GPU memory. This simply works by calling `.to(device)` on the model and tensors, where `device` and either be `cpu` or `cuda` (for the GPU)."
      ],
      "metadata": {
        "cell_id": "de1bb8eda67644db93779c72899e588c",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 220.734375,
        "id": "WKMcNCFUpORt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "19af38a10aa64403b865dde3603d43e3",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 169,
        "id": "8iRz6ZC7pORu"
      },
      "source": [
        "DEVICE = 'cpu' # later replace with 'cuda' for GPU\n",
        "EPOCHS = 3\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "cell_id": "2f12827ec5ad4a4ba18ae8d260857cd8",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "8kWWHxb1pORv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what's the accuracy is of our model. Since we already implemented accuracy in the previous exercise, we'll now let you use the torchmetrics package."
      ],
      "metadata": {
        "cell_id": "23af9a15c9e84bd3bea4a62c7c7e9459",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "7TWPAFvFpORv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "18891986243d49afa9305716cbd96aa7",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 97,
        "id": "6ElKGbYmpORw"
      },
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "accuracy = Accuracy(average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the average accuracy of all examples in the test dataset."
      ],
      "metadata": {
        "cell_id": "1cd5f69a8dcd41f8918bc9946a1b57e3",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "SKdTakYSpORw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "f73337d4e25a494e9c1b138823bc1d22",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 61,
        "id": "YUd9H43EpORx"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also look at the accuracy **for each class separately**:"
      ],
      "metadata": {
        "cell_id": "d8a4e62ebfb64dc5a6659df362eaeb50",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "JyOpnkgQpORx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "98b6c3390d1b42d2bd9b2e91608625aa",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 61,
        "id": "oKeCaSZgpORy"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embeddings"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c92bfb82034344bb9cd220de8e4e157c",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h2",
        "id": "TG0fqk_apORy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism does not consider the position of the tokens which hurts its performance for many problems. We can solve this issue in several ways. We can either add a positional encoding (via trigonometric functions) or we can learn positional embeddings along the way, in a similar way as BERT does. Here, we will add learnable positional embeddings to our exisisting model with another embedding layer."
      ],
      "metadata": {
        "cell_id": "070435ca57a94f1ea6e71050d57619aa",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 119.5625,
        "id": "Q7EM7zGdpORy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The longest sequence in our dataset has 78 tokens (you can trust us on that). So, let's set the number of embeddings for our positional embedding layer to that number. Again, you should use `nn.Embedding`."
      ],
      "metadata": {
        "cell_id": "5ff82560d6f24630a9bed80d6b38b7f1",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "W9HUerk7pOSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the inner parts of your `Transformer` class and add positional embeddings to it."
      ],
      "metadata": {
        "cell_id": "0363608d5a9a48f5be6f2e6ddbd7397c",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "RPqh5asxpOSK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "129e0b2b46c8494c90491fe2d2e137f0",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 313,
        "id": "Zp6iSw4rpOSL"
      },
      "source": [
        "class TransformerPos(nn.Module):\n",
        "    def __init__(self, emb_n: int, pos_emb_n: int, hidden_n: int, n:int =3, h:int =2):\n",
        "        \"\"\"\n",
        "        emb_n: number of token embeddings\n",
        "        pos_emb_n: number of position embeddings\n",
        "        hidden_n: hidden dimension\n",
        "        n: number of layers\n",
        "        h: number of heads per layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.positional_embeddings = ...\n",
        "        ...\n",
        "\n",
        "    def forward(self):\n",
        "        ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "b398b56387f54999966ee9d22a0150b1",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 61,
        "id": "bpR_ON4MpOSP"
      },
      "source": [
        "model_pos = CoNLL2000Transformer(TransformerPos(...), ...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "cell_id": "d8f6b79fce144176832210f7a1494288",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "8dQriOaJpOSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same procedure as before. Let's reinitialize our optimizer and our loss function and run the same training loop with our new model `model_pos`."
      ],
      "metadata": {
        "cell_id": "507700c451e842c4b9dbdd02857b234d",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "1U37j-KwpOSR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "7b2c6541371f4e99bb0acfe1212c732d",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 79,
        "id": "t3ZO1HXppOSS"
      },
      "source": [
        "optimizer = ....\n",
        "criterion = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "fac3717095914132b36cbfd70c702715",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 61,
        "id": "w_PuSECFpOST"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "cell_id": "2dfedd24910c45689ed2a3d4ed804af6",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "U2lnOjv3pOST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check if our performance on the accuracy got improved."
      ],
      "metadata": {
        "cell_id": "472e05fab0d149b8b269a12d8e363b85",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "GFFi_8DOpOST"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "38e0fe847e464f59b1861a749fc43811",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 61,
        "id": "ErAMnxfZpOSU"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's also check each class. Which classes got improved the most by adding positional embeddings?"
      ],
      "metadata": {
        "cell_id": "a49fe34010574a0096a020ead994158d",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "KkcUg_bFpOSU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "d107d15cf5fe4ee396f6733e093873cc",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 61,
        "id": "AyfXP2GNpOSV"
      },
      "source": [
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last question in this assignment doesn't require you to code anything. Instead, you're asked to point out possible issues with our current approach and name potential improvements. \n",
        "* ...\n",
        "* ...\n",
        "* ..."
      ],
      "metadata": {
        "cell_id": "bfece568a8c1460fafd968adf1972f19",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 175.953125,
        "id": "Zv6hz5NqpOSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=faa4af3b-d086-4f42-8b7d-d29c91b1d0f6' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ],
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "LwD3wZunpOSW"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {},
    "deepnote_notebook_id": "df3e6be7-b747-492e-86ed-19bc62a6bb4c",
    "deepnote_execution_queue": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  }
}